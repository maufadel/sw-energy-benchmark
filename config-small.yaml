# GPU configuration
CUDA_VISIBLE_DEVICES: "0"

# Benchmark settings
ITERATIONS: 5 #5
MAX_TEST_DURATION: 300 #300  # 5 minutes in seconds

# Server experiment settings
LAMBDA_QPS_ARRAY:
  - 0.014467592592592592
  - 0.38580246913580245

# LLM models to benchmark
LLM_MODELS:
  - "google/gemma-2-2b-it"
  - "google/gemma-2-9b-it"
  - "meta-llama/Llama-3.1-8B-Instruct"
  - "meta-llama/Llama-3.2-3B-Instruct"
  - "meta-llama/Llama-3.2-1B-Instruct"
  - "mistralai/Mistral-7B-Instruct-v0.3"
  - "deepseek-ai/deepseek-llm-7b-chat"
  - "Qwen/Qwen2.5-0.5B-Instruct"
  - "Qwen/Qwen2.5-1.5B-Instruct"
  - "Qwen/Qwen2.5-3B-Instruct"
  - "Qwen/Qwen2.5-7B-Instruct"

#  - "Qwen/Qwen3-32B"
#  - "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
#  - "google/gemma-2-27b-it"
