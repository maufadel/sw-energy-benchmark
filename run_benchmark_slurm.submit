#!/bin/bash
#SBATCH --time=600
#SBATCH --job-name=energy_bench
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --account=init
#SBATCH --gres=gpu:1
#SBATCH --output=/cluster/home/fues/sw-energy-benchmark/log/%j_%N__energy_bench.out
#SBATCH --error=/cluster/home/fues/sw-energy-benchmark/log/%j_%N__energy_bench.err

# Set the root folder for the project
ROOT_FOLDER="/cluster/home/fues/sw-energy-benchmark"
UV_PATH="/cluster/home/fues/.local/bin/uv"

# Check if a config file is provided, otherwise use default
CONFIG_FILE=${1:-config.yaml}

echo "Using config file: $CONFIG_FILE"

# --- Script Start ---
# SLURM provides $SLURM_JOB_ID as a unique identifier for each job.
echo "[$(date)] Starting Job ID: $SLURM_JOB_ID"

# Create a job-specific virtual environment in a temporary directory
VENV_DIR="/tmp/sw-energy-benchmark-venv-$SLURM_JOB_ID"
mkdir -p "$VENV_DIR"
"$UV_PATH" venv "$VENV_DIR"
source "$VENV_DIR/bin/activate"

# Set up a trap to clean up the virtual environment on exit
trap 'rm -rf -- "$VENV_DIR"' EXIT


# --- GPU-Specific Dependency Installation ---

# First, check if nvidia-smi is available to prevent errors
if ! command -v nvidia-smi &> /dev/null
then
    echo "nvidia-smi command not found. Cannot detect GPU type. Exiting."
    exit 1
fi

# Detect the GPU name. We only query for one GPU since we requested one (--gres=gpu:1).
# The `| head -n 1` ensures we only get one line of output.
GPU_NAME=$(nvidia-smi --query-gpu=gpu_name --format=csv,noheader,nounits | head -n 1)
echo "Detected GPU: $GPU_NAME"

# Default requirements file
REQS_FILE="$ROOT_FOLDER/requirements_modern.txt"

# Choose the requirements file based on the detected GPU name
# Using wildcards (*) makes the matching more robust (e.g., matches "NVIDIA A100-SXM4-40GB")
case "$GPU_NAME" in
  *"V100"*)
    echo "Found Tesla V100. Using modern requirements."
    REQS_FILE="$ROOT_FOLDER/requirements_modern.txt"
    ;;
  *"T4"*)
    echo "Found Tesla T4. Using modern requirements."
    REQS_FILE="$ROOT_FOLDER/requirements_modern.txt"
    ;;
  *"A100"*)
    echo "Found NVIDIA A100. Using modern requirements."
    REQS_FILE="$ROOT_FOLDER/requirements_modern.txt"
    ;;
  *"H100"*)
    echo "Found NVIDIA H100. Using modern requirements."
    REQS_FILE="$ROOT_FOLDER/requirements_modern.txt"
    ;;
  *"H200"*)
    echo "Found NVIDIA H200. Using modern requirements."
    REQS_FILE="$ROOT_FOLDER/requirements_modern.txt"
    ;;
  *)
    echo "Warning: Unrecognized GPU '$GPU_NAME'. Falling back to modern requirements file: $REQS_FILE"
    # The default is already set, so no action is needed here.
    ;;
esac

# Check if the selected requirements file exists
if [ ! -f "$REQS_FILE" ]; then
    echo "Error: Requirements file not found at $REQS_FILE"
    exit 1
fi

# Install dependencies into the virtual environment using the selected file
echo "Installing dependencies from: $REQS_FILE"
"$UV_PATH" pip install -r "$REQS_FILE"

echo "Dependencies installed successfully."

# Discover the correct library path within the venv
# --- START DEBUGGING BLOCK ---
echo "--- [DEBUG] Verifying library installation ---"

# Define the site-packages path for easier access
SITE_PACKAGES="$VENV_DIR/lib/python3.11/site-packages"

echo "[DEBUG] VENV_DIR is: $VENV_DIR"
echo "[DEBUG] Searching for libcusparseLt.so.0 in the entire venv..."

# Search the entire virtual environment for the missing library
find "$VENV_DIR" -name "libcusparseLt.so.0"

echo "[DEBUG] Listing contents of the expected directory..."
# List the contents of the directory where the library SHOULD be
ls -l "$SITE_PACKAGES/nvidia/cusparse/lib/"

echo "[DEBUG] Current LD_LIBRARY_PATH is: $LD_LIBRARY_PATH"
echo "--- [DEBUG] End of verification ---"


export LD_LIBRARY_PATH="$SITE_PACKAGES/nvidia/cusparselt/lib:$SITE_PACKAGES/nvidia/cublas/lib:$SITE_PACKAGES/nvidia/cusparse/lib:$SITE_PACKAGES/nvidia/cudnn/lib:$LD_LIBRARY_PATH"

echo "[DEBUG] New LD_LIBRARY_PATH is: $LD_LIBRARY_PATH"
echo "--- [DEBUG] End of verification ---"

# Set up a job-specific Hugging Face home to isolate model downloads
HF_HOME_DIR="/cluster/home/fues/.cache/huggingface"
mkdir -p "$HF_HOME_DIR"
export HF_HOME="$HF_HOME_DIR"


echo "[DEBUG] Current HF_HOME_DIR is: $HF_HOME_DIR"


# Set up a job-specific vLLM cache
#VLLM_CACHE_ROOT="/tmp/vllm-$SLURM_JOB_ID"
#mkdir -p "$VLLM_CACHE_ROOT"
#export VLLM_CACHE_ROOT="$VLLM_CACHE_ROOT"

echo "[DEBUG] Current VLLM_CACHE_ROOT is: $VLLM_CACHE_ROOT"


## DOWNLOADING MODELS BEFORE
# --- Configuration ---
ENV_FILE=".env"

# --- Script Start ---

# 1. Check if the YAML file exists
if [ ! -f "$CONFIG_FILE" ]; then
    echo "Error: YAML file not found at '$CONFIG_FILE'"
    exit 1
fi

# 2. Check if the .env file exists and load the token
if [ -f "$ENV_FILE" ]; then
    # Using 'set -a' and 'set +a' is a robust way to export variables from the file
    set -a
    source "$ENV_FILE"
    set +a
    echo "Loaded HF_TOKEN from $ENV_FILE."
else
    echo "Warning: .env file not found. Will try to proceed without a token."
    echo "This may fail for gated models like Llama or Gemma."
fi

# 3. Check if HF_TOKEN is set in the environment
if [ -z "$HF_TOKEN" ]; then
    echo "-----------------------------------------------------"
    echo "Warning: HF_TOKEN is not set."
    echo "Gated models will likely fail to download."
    echo "If you need them, create a .env file with HF_TOKEN=your_token"
    echo "-----------------------------------------------------"
fi


# 4. Parse the YAML file and download each model
echo "Parsing models from $CONFIG_FILE..."

# Use awk to find the LLM_MODELS block and extract model names.
# This is more robust for complex YAML files and ignores commented-out lines.
MODELS_TO_DOWNLOAD=$(awk '
  # Start processing when we see the LLM_MODELS key
  /^LLM_MODELS:/ { in_models_block = 1; next }
  # Stop processing if we see another top-level key
  /^[A-Z_]+:/ { in_models_block = 0 }
  # If in the block, the line is a model entry, and not a comment...
  in_models_block && /^\s*-\s*"/ && !/^\s*#/ {
    # ...extract the string inside the quotes.
    match($0, /"([^"]+)"/);
    if (RSTART) {
      print substr($0, RSTART + 1, RLENGTH - 2);
    }
  }
' "$CONFIG_FILE")


for model in $MODELS_TO_DOWNLOAD; do
    echo "====================================================="
    echo "Preparing to download: $model"
    echo "====================================================="

    # Use huggingface-cli to download.
    huggingface-cli download "$model" --token "$HF_TOKEN" 

    # Check if the download was successful
    if [ $? -eq 0 ]; then
        echo "--> SUCCESS: Successfully downloaded $model."
    else
        echo "--> ERROR: Failed to download $model. Please check logs for details."
    fi
done

echo ""
echo "====================================================="
echo "All model downloads attempted. Process finished."
echo "====================================================="






# Set up a trap to clean up the virtual environment and HF home on exit
#trap 'rm -rf -- "$VENV_DIR" "$HF_HOME_DIR" "VLLM_CACHE_ROOT"' EXIT
#trap 'rm -rf -- "$VENV_DIR"' EXIT

# --- Create a unique results directory for this specific job ---
# This prevents multiple jobs from overwriting each other's results.
JOB_RESULTS_FOLDER="$ROOT_FOLDER/results/$SLURM_JOB_ID"
mkdir -p "$JOB_RESULTS_FOLDER"
echo "[$(date)] Results will be stored in: $JOB_RESULTS_FOLDER"

# Define the path for the system info file within the unique job folder
RESULTS_FILE="$JOB_RESULTS_FOLDER/system_info.txt"

# ---------------------------------------
# Register system info.
# ---------------------------------------
echo "[$(date)] Registering system info..."

# The > operator creates and overwrites the file with the first command's output.
# The >> operator appends the output of subsequent commands to the same file.

echo "lscpu" > "$RESULTS_FILE"
lscpu >> "$RESULTS_FILE"
echo -e "\n" >> "$RESULTS_FILE"

echo "nvidia-smi" >> "$RESULTS_FILE"
nvidia-smi >> "$RESULTS_FILE"
echo -e "\n" >> "$RESULTS_FILE"

echo "free -h" >> "$RESULTS_FILE"
free -h >> "$RESULTS_FILE"
echo -e "\n" >> "$RESULTS_FILE"

echo "df -h" >> "$RESULTS_FILE"
df -h >> "$RESULTS_FILE"
echo -e "\n" >> "$RESULTS_FILE"

echo "[$(date)] System info registered in $RESULTS_FILE"

# ---------------------------------------
# Run the Python scripts for language
# You can redirect the output of each script to a specific file in your job's result folder
echo "[$(date)] Running llm_server_optimized.py..."
#srun python "$ROOT_FOLDER/language/llm_server_optimized.py" "$JOB_RESULTS_FOLDER" > "$JOB_RESULTS_FOLDER/llm_server_optimized.out"
srun --export=ALL python "$ROOT_FOLDER/language/llm_server_optimized.py" "$JOB_RESULTS_FOLDER" --config "$CONFIG_FILE" > "$JOB_RESULTS_FOLDER/llm_server_optimized.out"

echo "[$(date)] Running llm_batch_optimized.py..."
#srun python "$ROOT_FOLDER/language/llm_batch_optimized.py" "$JOB_RESULTS_FOLDER" > "$JOB_RESULTS_FOLDER/llm_batch_optimized.out"
srun --export=ALL python "$ROOT_FOLDER/language/llm_batch_optimized.py" "$JOB_RESULTS_FOLDER" --config "$CONFIG_FILE" > "$JOB_RESULTS_FOLDER/llm_batch_optimized.out"

# ---------------------------------------
# Run the Python scripts for vision
#echo "[$(date)] Running resnet50_batch.py..."
#python vision/resnet50_batch.py > "$JOB_RESULTS_FOLDER/resnet50_batch.out"
#echo "[$(date)] Running resnet50_server.py..."
#python vision/resnet50_server.py > "$JOB_RESULTS_FOLDER/resnet50_server.out"

# ---------------------------------------
# Run the Python scripts for tabular
#echo "[$(date)] Running tabular_batch.py..."
#python tabular/tabular_batch.py > "$JOB_RESULTS_FOLDER/tabular_batch.out"
#echo "[$(date)] Running tabular_server.py..."
#python tabular/tabular_server.py > "$JOB_RESULTS_FOLDER/tabular_server.out"


echo "[$(date)] Job Finished"

